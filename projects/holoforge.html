<html>
    <head>
        <title>yhawaz</title>
    <style>
    li {
        padding-top: 7px;
    }
    #bodydiv {
      padding: 20px;
      max-width: 1000px;
      /*max-width: 50em;*/
      margin-left: auto;
      margin-right: auto;
      border-radius: 10px;
    }
    pre {
      background-color: #eee;
      padding:10px;
      border-radius: 5px;
      overflow-x: auto;
    }
    h2 {
      margin-top: 2em;
    }

    </style>
    </head>
    <body>
        <div id="bodydiv">
            <h1>holoforge</h1>
            <h3>Hand Controlled 3D Model Viewer on FPGA, it was sorta inspired by iron man</h3>
            <h4><a href="https://youtu.be/Jawq5mXmuF8">https://youtu.be/Jawq5mXmuF8"</a></h4>

      <img  src="https://img.youtube.com/vi/Jawq5mXmuF8/maxresdefault.jpg"  align ="left" height= "250px" alt="check out the video cause words are fake" />
        <img src="../photos/holoforge_state.png" align= "middle" alt="Crosshair view indicating blue-pixel center of mass" height="250px" />
      <p>This was our final project for MIT’s Digital Systems Class (6.111/6.2050). In this class, all the work was done in SystemVerilog and o Xilinx Spartan 7 FPGAs. The class concluded with an open-ended final project, and we decided to create a camera-controlled 3D model viewer that could render 3D meshes and change the view of the scene based on real-time camera inputs. We also decided(were forced) to utilize the offboard DDR3 RAM for our framebuffer to allow for better resolution down the line, also because we were already fully utilizing the BRAM for loading models.</p>
        <img src="../photos/cube.png" align="left" alt="Wireframe cube rendered by HoloForge" height="200px" />
        <img src="../photos/crosshair.png" align= "middle" alt="Crosshair view indicating blue-pixel center of mass" height="200px" />
      <p>We currently have successful rendering of 3D objects (like the cube shown) and the ability to change the view of the object based on the center of mass of all the blue pixels on the screen observed via an Adafruit camera hooked into the FPGA. We also have a “crosshair screen” that lets the user know what the center of mass of blue is (i.e., where their “virtual” cursor is). See the video above for a better explanation.</p>

      <p>The reason we decided to use a MIG with AMBA AXI was that it made it easier to write out-of-order addresses to the MIG via a custom stacker we wrote. This stacker takes the 16-bit color values that come out of the graphics pipeline and stacks them into 128 bits (with varying strobes based on whether the data is in order) that get fed into the MIG. We have two frames in the DRAM for clearing and switching, and we have working view changes with the camera. For more information, read the report—note we implemented a portion of features after the class was over.</p>


        </div>
    </body>




</html>
